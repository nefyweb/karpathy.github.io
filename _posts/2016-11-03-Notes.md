Interesting for dropping stuff not counting certain values using stack:

df = df_2
threshold = 95 # Anything that occurs less than this will be removed.
value_counts = df["Department"].value_counts() # Specific column 
to_remove = value_counts[value_counts <= threshold].index
df["Department"].replace(to_remove, np.nan, inplace=True)


df_2.loc[0, 'Blue'] = df_2.loc[0, 'Amount']

Interesting, with the above you can create a new column with the .loc function, the zero selects the first items to read/write to.

df.loc[0, 'Under'] = 1 if (df.loc[1]['Amount'] < df.loc[0]['Amount']) else 0
# This if statemetn is much easier

How to get a unique list of all the items in a column: line_order = list(df["LineOrder"].unique())

Interesting way of creating a dataframe the saize that you want to be, but you would probably be better off just appending.

#frame = pd.DataFrame(0, index=np.arange(len(df)), columns=feature_list)

```python
http://mybinder.org:/repo/snowde/templates

This is an interesting where command, that says if equal, then x else y.

mom1 = np.where(mom1 > 0, 1, 0)

Interesting: You can create the below method to create a dataframe out of a group by object:

grouped = blue.groupby(["fyearq", "fqtr"],as_index=False).count()

This approach is much better in creating a dataframe ou tof a group by object:

grouped = blue.groupby(["fyearq", "fqtr"])

for key, item in grouped:
    print grouped.get_group(key), "\n\n"
    
Another method:

df.groupby('A').apply(lambda x: x)

NB the last value in brackets is inportant because you have to identify which values you want ot keep in the dataframe

grouped = blue.groupby(["fyearq", "fqtr"])[["mom"]]

bloom = train_new.reset_index()
bloom.reindex(train_new["id"])
train_new = train_new.fillna(a_mean)
id_df = train_new[col].groupby('id').agg([np.mean, np.std, len]).reset_index()

grouped = merged.groupby(["fyearq", "fqtr", "ticker"]).agg([np.mean, np.sum, np.std, len]).reset_index()


for o in range(len(train_new)):
id_df = bloom[col].groupby('id')
a_mean = id_df.mean().reset_index()
train_new = train_new.fillna(a_mean)
sum(train_new.isnull().sum(axis=1))

train["std"] = train.groupby('id').y.std()
corr = train.corr().ix["y", "std"]

merged = pd.merge(price, fiscal, on="date", how="outer")


# NB ix is the pandas slicer. 
fire1 = fire.ix[[:0]]
fire2 = fire.ix[[2:]]
newfire = pd.concat([fire1, fire2], axis=1)

# NB this drops the second row heade
fire.columns = fire.columns.droplevel(1)

-> This is interesting, reminds you that you have to 
pd.merge(left, right, left_on='key1', right_on='key2')

# Interesting, merge and joing essentially means the same thing,
# slightly differnt way of writing it out. 

And then concatante the one that just copies and paste. 


Here I am first creating a separate datafrane, 

d = pd.DataFrame()
dates = pd.date_range('20040101', '20170101')
s = pd.Series(dates)
d["fdate"] = s

This is how you can create awesome files from /// to yyyymmdd
TEXT(A1,"yyyymmdd")  and then drag it down. 

There is a way to play around with excel in pandas.

import openpyxl

d = pd.DataFrame()
dates = pd.date_range('2004/01/01', '2017/01/01')

mask in dataframe is like where. (IF THEN statement)

The statement I am looking for is ht emap function, which maps for each x value.

prices["cusip"]= prices["cusip"].map(lambda x: x[:5] + '10')

MAP FOR SERIES, APPLY FRO FRAME

apply is on id arrays

Wow this is an awesom .isin list code:

prices = prices[~prices["tic"].isin(tickel)]

Wow there actually is a way to get numpy into dataframe:

df = pd.DataFrame({'mom1':mom1.tolist()})

Here is an intresting location function, this helps to achieve good things:

d["close"] = new_group["close"].loc[new_group["year"]>"2004"]

When you get this issue::: 

only integers, slices (`:`), ellipsis (`â_|`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

Then you can fix it by first specifying a dataframe beforeyou do anything else:

mom2 = pd.DataFrame()
mom2['price_above'] = np.where(values > 0, 1, 0)

Melt is one of the most beautiful function that I have ever seen, it stacks columns.

keys = [c for c in mom_vol_n if c.startswith('mom_vol_')]
new = pd.melt(mom_vol_n, value_vars=keys, value_name='mom_v')


NB when you are running loops over a dictionary in python 2 then use
for i, v in varlist.iteritems():

in python 3 you only have to use:

for i, v in varlist.items():

But, you have to inlcue items. 


You can find dtyoe of column doing the following:

pandas["column"].dtype
no ()


Awesome this code is good for appen in for :

   framed = np.append(framed, [train_predictions], axis=0)


To force division to be float

from __future__ import division

To pass on certain type of warnings:

def warn(*args, **kwargs): pass


For max values
numpy.set_printoptions(threshold='nan')


load ./zero_cleaner.py
This is an interesting line of code, it actually helps you to visualy load any .py into your document.

There is even a way to pass variables between netbooks, see this link:

https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/

Awesome you can us the follwoing magic command to write a cell to a python 
document: 

%%writefile pythoncode.py


This is a way to load in your py code without visually showing the code:

%run zero_cleaner.py

Also if you compilef the class file in the same python version you can use it as a class by chanign it to .pyc and just imorting it with just using the name
zero_cleaner

normalized_df=(coy-coy.min())/(coy.max()-coy.min())

The below is a long approach that simply does what is done above:

scaler = preprocessing.MinMaxScaler()
scaled = scaler.fit_transform(values)
coy = pd.DataFrame(scaled)

Interesting, you can .value an entire dataframe:

values = coy.values

NB when merging items, makes sure that you are mergin items of the same type.

You normally import in string, meaning that your next iports should also be in string. You can import one file in string and the other one in auto format i.e. leave to default, and then group the frame together later on.

This is how to import string 

fiscal = pd.read_csv("fiscal_bo.csv", dtype=str)

how : {â_˜anyâ_™, â_˜allâ_™}
any : if any NA values are present, drop that label
all : if all values are NA, drop that label

How to install talib package: 
ta-lib: 0.4.9-np19py27_0 quantopian

conda install -c quantopian ta-lib=0.4.9

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

This is probably a better way to write the merged file or any big file, just cut it off :

data = pd.merge(data, nRuta_SAK, 
                            how='left',
                            left_on=['Ruta_SAK'], 
                            right_on=['Ruta_SAK'],
                            left_index=False, right_index=False, sort=True,
                            suffixes=('_x', '_y'), copy=False) 

This is what I have: 

py27 -> py27na (This is the one without talib possibly?) 

ta-lib: 0.4.9-np19py27_0 quantopian
I found a way to clone an environement: this means that you can have similar running packages:

dereksnow$ conda create -n py27na --clone py27

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

In terms of website: 

Nameserver changes are heavier, dns is less heavy.

Nameserver means that the full domain sits with someone else.

Nameserver changes host.

DNS only gives another the right to render I believe.

I actually want to read up on some of these stuff, and find out how much they are different. 

The following is some interesting Conda Code:

conda search python

conda install python=xxx.xxx

python --version

conda create -n py35 python=3.5 anaconda

conda update python

source activate py36

source deactivate py36

conda create -n py27 python=2.7 anaconda
conda create -n 27clean python=2.7 anaconda
source activate py27
conda install notebook ipykernel
ipython kernel install --user
ipython kernel install --user --name t35
(I screwed this one up)

How to open bash files for editing:

touch ~/.bash_profile; open ~/.bash_profile

source activate py27clean

You have to add the anaconda at the back otherwise the thing gets confused. 

# To activate this environment, use:
# > source activate 27clean
#
# To deactivate this environment, use:
# > source deactivate 27clean
#

# To activate this environment, use:
# > source activate 27
#
# To deactivate this environment, use:
# > source deactivate 27talib


Another thing to be very cautious of is what kernel you are using in ipython.

So far I only have these environments: 
27clean, 27talib etc that is worth using. 

inputs = {
    'open': open_1,
    'high': high,
    'low': low,
    'close': close,
    'volume': volume
}
# Wow this is interesting, you can specify the input arrays in the hope that
# the function will automatically pick them up.
# I like this idea, everything will process much quicker.



What this is awesome, there is actually a way to loop in parrallel, i.e. a simultaneous loop.

for f, b in zip(foo, bar):
    print(f, b)
    
You don't ever really have to change the variable name, you can just change the column name:

saved[lab] = i(inputs)

The approach to get_dummmies. 

Fuck that, i just created my own dummies:

    saved[b+'false_'+ cusi] = np.where(i(inputs) >99,1,0)
    saved[b+'true'+ cusi] = np.where(i(inputs) <-99,1,0)


But there is another way to do get dummies and this is with this procedure:

cols_to_transform = ['year', 'qtr']
frame = pd.get_dummies(avg, columns = cols_to_transform)
frame = pd.concat((frame, avg), axis=1)

This is how to fill numpy nan's with zeros:

here[np.isnan(here)] = 0

At the moment I have done min max scaling because it is noteably good for neural networks. If I find that the results are not to good, I might also look at producing standarisation techniques. 

You can't really do them together I have thought about that now. 

Can use command plus the arrow buttons to get to the end of along line. A lot like excel in fact. 

That is very interesting, instead of forcing a shit load of new variables, you can instead store all these "variables" away in a dictionary, this leads to much more efficent code. It kind of looks like a dataframe when you do this. 

NB there is a big distinction between append in lists: 

Append is essentially in a loop where each new item such as "burger", "pizza" etc gets appended one after the other in a long list.

However, when you have a pre-established list: list area = ["burger", "pizza"]
and blue = ["diamond", "liament"] then:
blue = blue + area to keep making a long list where one follows the other.

So the above should not use the append function. 

However, it does seem that you can add the function extend instead of the plus:

x.extend([4, 5])

This is how to do that fancy formatting printing:

print("%d. feature %d and %str (%f)" % (f + 1, indices[f], list(X)[indices[f]], importances[indices[f]]))


It's simply pointless to create variable variable names. Why?

They are unnecessary: You can store everything in lists, dictionarys and so on
They are hard to create: You have to use exec or globals()

How to select a certain cell in a pandas dataframe:

train.iloc[0]["photos"]

JSON's upload is tricky in that it is a little bit differnt to read:

# train = pd.read_json("train.json", "r")
You need the r, otherwise it does not give you all the level that you need. 

NB you can change the bash profile in just one line:

export PATH="/home/username/miniconda/bin:$PATH"

Replace is much better than map for replacing values, map actually only tajks int the values available.

df["Year"].replace(["2011/12","2010/11","2009/10","2008/09"],["2012","2011","2010","2009"], inplace=True)




http://mybinder.org:/repo/snowde/templates

This is an interesting where command, that says if equal, then x else y.

mom1 = np.where(mom1 > 0, 1, 0)

Interesting: You can 
the below method to create a dataframe out of a group by object:

grouped = blue.groupby(["fyearq", "fqtr"],as_index=False).count()

This approach is much better in creating a dataframe ou tof a group by object:

grouped = blue.groupby(["fyearq", "fqtr"])

for key, item in grouped:
    print grouped.get_group(key), "\n\n"
    
Another method:

df.groupby('A').apply(lambda x: x)

NB the last value in brackets is inportant because you have to identify which values you want ot keep in the dataframe

grouped = blue.groupby(["fyearq", "fqtr"])[["mom"]]

bloom = train_new.reset_index()
bloom.reindex(train_new["id"])
train_new = train_new.fillna(a_mean)
id_df = train_new[col].groupby('id').agg([np.mean, np.std, len]).reset_index()

grouped = merged.groupby(["fyearq", "fqtr", "ticker"]).agg([np.mean, np.sum, np.std, len]).reset_index()


for o in range(len(train_new)):
id_df = bloom[col].groupby('id')
a_mean = id_df.mean().reset_index()
train_new = train_new.fillna(a_mean)
sum(train_new.isnull().sum(axis=1))

train["std"] = train.groupby('id').y.std()
corr = train.corr().ix["y", "std"]

merged = pd.merge(price, fiscal, on="date", how="outer")


# NB ix is the pandas slicer. 
fire1 = fire.ix[[:0]]
fire2 = fire.ix[[2:]]
newfire = pd.concat([fire1, fire2], axis=1)

# NB this drops the second row heade
fire.columns = fire.columns.droplevel(1)

-> This is interesting, reminds you that you have to 
pd.merge(left, right, left_on='key1', right_on='key2')

# Interesting, merge and joing essentially means the same thing,
# slightly differnt way of writing it out. 

And then concatante the one that just copies and paste. 


Here I am first creating a separate datafrane, 

d = pd.DataFrame()
dates = pd.date_range('20040101', '20170101')
s = pd.Series(dates)
d["fdate"] = s

This is how you can create awesome files from /// to yyyymmdd
TEXT(A1,"yyyymmdd")  and then drag it down. 

There is a way to play around with excel in pandas.

import openpyxl

d = pd.DataFrame()
dates = pd.date_range('2004/01/01', '2017/01/01')

mask in dataframe is like where. (IF THEN statement)

The statement I am looking for is ht emap function, which maps for each x value.

prices["cusip"]= prices["cusip"].map(lambda x: x[:5] + '10')

MAP FOR SERIES, APPLY FRO FRAME

apply is on id arrays

Wow this is an awesom .isin list code:

prices = prices[~prices["tic"].isin(tickel)]

Wow there actually is a way to get numpy into dataframe:

df = pd.DataFrame({'mom1':mom1.tolist()})

Here is an intresting location function, this helps to achieve good things:

d["close"] = new_group["close"].loc[new_group["year"]>"2004"]

When you get this issue::: 

only integers, slices (`:`), ellipsis (`â_|`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

Then you can fix it by first specifying a dataframe beforeyou do anything else:

mom2 = pd.DataFrame()
mom2['price_above'] = np.where(values > 0, 1, 0)

Melt is one of the most beautiful function that I have ever seen, it stacks columns.

keys = [c for c in mom_vol_n if c.startswith('mom_vol_')]
new = pd.melt(mom_vol_n, value_vars=keys, value_name='mom_v')


NB when you are running loops over a dictionary in python 2 then use
for i, v in varlist.iteritems():

in python 3 you only have to use:

for i, v in varlist.items():

But, you have to inlcue items. 


You can find dtyoe of column doing the following:

pandas["column"].dtype
no ()


Awesome this code is good for appen in for :

   framed = np.append(framed, [train_predictions], axis=0)


To force division to be float

from __future__ import division

To pass on certain type of warnings:

def warn(*args, **kwargs): pass


For max values
numpy.set_printoptions(threshold='nan')


load ./zero_cleaner.py
This is an interesting line of code, it actually helps you to visualy load any .py into your document.

There is even a way to pass variables between netbooks, see this link:

https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/

Awesome you can us the follwoing magic command to write a cell to a python 
document: 

%%writefile pythoncode.py


This is a way to load in your py code without visually showing the code:

%run zero_cleaner.py

Also if you compilef the class file in the same python version you can use it as a class by chanign it to .pyc and just imorting it with just using the name
zero_cleaner

normalized_df=(coy-coy.min())/(coy.max()-coy.min())

The below is a long approach that simply does what is done above:

scaler = preprocessing.MinMaxScaler()
scaled = scaler.fit_transform(values)
coy = pd.DataFrame(scaled)

Interesting, you can .value an entire dataframe:

values = coy.values

NB when merging items, makes sure that you are mergin items of the same type.

You normally import in string, meaning that your next iports should also be in string. You can import one file in string and the other one in auto format i.e. leave to default, and then group the frame together later on.

This is how to import string 

fiscal = pd.read_csv("fiscal_bo.csv", dtype=str)

how : {â_˜anyâ_™, â_˜allâ_™}
any : if any NA values are present, drop that label
all : if all values are NA, drop that label

How to install talib package: 
ta-lib: 0.4.9-np19py27_0 quantopian

conda install -c quantopian ta-lib=0.4.9

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

This is probably a better way to write the merged file or any big file, just cut it off :

data = pd.merge(data, nRuta_SAK, 
                            how='left',
                            left_on=['Ruta_SAK'], 
                            right_on=['Ruta_SAK'],
                            left_index=False, right_index=False, sort=True,
                            suffixes=('_x', '_y'), copy=False) 

This is what I have: 

py27 -> py27na (This is the one without talib possibly?) 

ta-lib: 0.4.9-np19py27_0 quantopian
I found a way to clone an environement: this means that you can have similar running packages:

dereksnow$ conda create -n py27na --clone py27

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

In terms of website: 

Nameserver changes are heavier, dns is less heavy.

Nameserver means that the full domain sits with someone else.

Nameserver changes host.

DNS only gives another the right to render I believe.

I actually want to read up on some of these stuff, and find out how much they are different. 

The following is some interesting Conda Code:

conda search python

conda install python=xxx.xxx

python --version

conda create -n py35 python=3.5 anaconda

conda update python

source activate py36

source deactivate py36

conda create -n py27 python=2.7 anaconda
conda create -n 27clean python=2.7 anaconda
source activate py27
conda install notebook ipykernel
ipython kernel install --user
ipython kernel install --user --name t35
(I screwed this one up)

How to open bash files for editing:

touch ~/.bash_profile; open ~/.bash_profile

source activate py27clean

You have to add the anaconda at the back otherwise the thing gets confused. 

# To activate this environment, use:
# > source activate 27clean
#
# To deactivate this environment, use:
# > source deactivate 27clean
#

# To activate this environment, use:
# > source activate 27
#
# To deactivate this environment, use:
# > source deactivate 27talib


Another thing to be very cautious of is what kernel you are using in ipython.

So far I only have these environments: 
27clean, 27talib etc that is worth using. 

inputs = {
    'open': open_1,
    'high': high,
    'low': low,
    'close': close,
    'volume': volume
}
# Wow this is interesting, you can specify the input arrays in the hope that
# the function will automatically pick them up.
# I like this idea, everything will process much quicker.



What this is awesome, there is actually a way to loop in parrallel, i.e. a simultaneous loop.

for f, b in zip(foo, bar):
    print(f, b)
    
You don't ever really have to change the variable name, you can just change the column name:

saved[lab] = i(inputs)

The approach to get_dummmies. 

Fuck that, i just created my own dummies:

    saved[b+'false_'+ cusi] = np.where(i(inputs) >99,1,0)
    saved[b+'true'+ cusi] = np.where(i(inputs) <-99,1,0)


But there is another way to do get dummies and this is with this procedure:

cols_to_transform = ['year', 'qtr']
frame = pd.get_dummies(avg, columns = cols_to_transform)
frame = pd.concat((frame, avg), axis=1)

This is how to fill numpy nan's with zeros:

here[np.isnan(here)] = 0

At the moment I have done min max scaling because it is noteably good for neural networks. If I find that the results are not to good, I might also look at producing standarisation techniques. 

You can't really do them together I have thought about that now. 

Can use command plus the arrow buttons to get to the end of along line. A lot like excel in fact. 

That is very interesting, instead of forcing a shit load of new variables, you can instead store all these "variables" away in a dictionary, this leads to much more efficent code. It kind of looks like a dataframe when you do this. 

NB there is a big distinction between append in lists: 

Append is essentially in a loop where each new item such as "burger", "pizza" etc gets appended one after the other in a long list.

However, when you have a pre-established list: list area = ["burger", "pizza"]
and blue = ["diamond", "liament"] then:
blue = blue + area to keep making a long list where one follows the other.

So the above should not use the append function. 

However, it does seem that you can add the function extend instead of the plus:

x.extend([4, 5])

This is how to do that fancy formatting printing:

print("%d. feature %d and %str (%f)" % (f + 1, indices[f], list(X)[indices[f]], importances[indices[f]]))


It's simply pointless to create variable variable names. Why?

They are unnecessary: You can store everything in lists, dictionarys and so on
They are hard to create: You have to use exec or globals()

How to select a certain cell in a pandas dataframe:

train.iloc[0]["photos"]

JSON's upload is tricky in that it is a little bit differnt to read:

# train = pd.read_json("train.json", "r")
You need the r, otherwise it does not give you all the level that you need. 


This is how you can install packages of spcecific version using pip ==

pip install xgboost==0.4a30

When xgboost says you can install somethign pip says fuckoff you can. 

You know creating these environments with new ipython kernels, always tst run it with python, import -> package.

This initialises it if you already have the notebook running. 

Another  thing I saw is that git clone ogten works for the specialised packages.


This is an interesting piece of code, to convert a text class to a numerical class, I assume it is faster than other methods:

y_map = {'low': 2, 'medium': 1, 'high': 0}
train['interest_level'] = train['interest_level'].apply(lambda x: y_map[x])

NB .values gives a numpy representation, to get series all you have to do is extract one column. 

Below is some interesting ways to extract, data attributes from a date:

train_test['Date'] = pd.to_datetime(train_test['created'])
# The one above here is quite interesting. 

train_test['Year'] = train_test['Date'].dt.year
train_test['Month'] = train_test['Date'].dt.month
train_test['Day'] = train_test['Date'].dt.day
train_test['Wday'] = train_test['Date'].dt.dayofweek
train_test['Yday'] = train_test['Date'].dt.dayofyear
train_test['hour'] = train_test['Date'].dt.hour


I quite like this one coders approach, which is to create new features and driop the dervivitave source features as he goes along. 

Jesus, this is a beautiful bit of code that tells you more about how much of each ctagory or items there are in a list in a series. It is not like .agg or groupby function that look at the entire dataframe. 

managers_count = train_test['manager_id'].value_counts()

Man honestly, I like this idea of creating percentile feauters, with count.values and percentiles, when what you are counting are text valued attributes. 

train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[
    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)
train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[
    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)

Interesting, the fact of the matter is that not all your columns in your dataframe are categorical, so what you can do is find the type and see if it is object, this would help you a lot:

categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']

for feat in categoricals:
    lbl = preprocessing.LabelEncoder()
    lbl.fit(list(train_test[feat].values))
    train_test[feat] = lbl.transform(list(train_test[feat].values))

Label encoder basically just takes the text and transforms it into 1- 33445 whatever number n is [Encode labels with value between 0 and n_classes-1..]

Therefore label encoder is the opposite of what pd.get_dummies do

When you want to check the type of all the cloumns in the dataframe you woudl have to loop. When you only want to check a column or a series you can just use the dtype command.

for col in df:
    print col, df[col].dtypes
    
Size of columns and rows. 
pd.set_option('display.height', 500)
pd.set_option('display.max_rows', 500)

test_name: name of corresponding column in test dataframe;
train_series and test_series: actual columns (as pandas Series).

factorize will return, in second position, the list of unique values (or categorical labels) in the provided column, and, in first position, the indices that would let you recreate the original column from the unique values. In other words:

# The bottom basically returns how many values it has in th ebathroom cat:

train_test['bathrooms_cat'], labels = pd.factorize(train_test['bathrooms_cat'].values, sort=True)
train_test.drop('bathrooms', axis=1, inplace=True)

features += c_vect_sparse1_cols
# same as  features = features + c_vect_sparse1_cols

This is a much more legit way in finding out whether you have any missing data:
msno.matrix(data,figsize=(13,3))

You fuck the shape thing up the reaoson being that you want to call a method with parenthesis, what you reall y want is an attribute, hence it should be written as follows:

y_train.shape

NB instead of .valuing for numpy you can just slice it will also turn into an array:

y_train = y_train[:3000]

I like the idea of editing the data all at once for both train and test, instead of doing it separately, however for some stuff like reinforcement learning, there is a requirement to do it in a while loop which is not the same.

I expect all of this to become 10,000 lines of code. 

Instead of looping over dtype you can find some more info on a pandas dataframe and get datatypes simply by doing the following:

preds.info()

Here: This is how to create a headerless mardown table:

|     |    
|-----|

` ` Creates this interesting highlight format. 

Something interesting about string imports, you do not have to import all fields to string, becuase that has caused me some serious issues in the past, you could rather import as string one by one as below:

limit_rows   = 7000000
df = pd.read_csv("../input/train_ver2.csv",dtype={"sexo":str,
                                                    "ind_nuevo":str,
                                                    "ult_fec_cli_1t":str,
                                                    "indext":str}, nrows=limit_rows)

Another intersting approach the above import took is to limit the amount of rows used to import. 

The following is the code needed for a series of unique id's:

unique_ids   = pd.Series(df["ncodpers"].unique())

Interesting the below is an attempt to limit the number of unique id's/people, that you want to keep in the dataframe:

unique_id    = unique_ids.sample(n=limit_people)

Also never forget about the isin / is in command: 

df           = df[df.ncodpers.isin(unique_id)]

The above line of code is significantly going to clamp don on the size of the dataframe in the sense that only a certain amount of people are being kept. 

Remember that df.describe is going to tell you if there is null vlues by looking at the percentiles, due the calculation not going through.

df.describe()

This little piece of code would do the same, I think it might be the preferred way:

df.isnull().any()
-> Remember that 0.000 is not null, null is NaN. 

Look at this beautifull to datetime function that helps to take a ranodm date column and turn it into a date you would like, date formatting. 

df["fecha_dato"] = pd.to_datetime(df["fecha_dato"],format="%Y-%m-%d")

So it seems that for tablaeu, the idea is that you can use certain data extraction techiques in python that would help you use that data in tableu, you cant use it in jupyter notebooks. 

So this guy, show the approach of filling all missing values, feature by feature, to identify legitimacy of missing values. 

Remember how to find out how many null values there are:

df["ind_nuevo"].isnull().sum()

That is interesting, if you don't want tot exclude the null values from the dataframe just yet, you can use this t call up the non-null values.

df.renta.notnull()

How to create a category out of a name of a province: personally, I don't know why this is done, but it does get done.

incomes.nomprov = incomes.nomprov.astype("category", categories=[i for i in df.nomprov.unique()],ordered=False)

NB very important, to do the following, if you want to give certain categories median values specifically, i.e. a better way to fill null values.

So basically what is done is that the groupby group created, called grouped that has the median of the category name, is essentially merged innerly, so the new renta gets filled only with median values where existing values do not exist. 

Null's can't just be formed randomly, it needs some human knowledge and intuition to actually do anytype of worthwhil munding. 

grouped        = df.groupby("nomprov").agg({"renta":lambda x: x.median(skipna=True)}).reset_index()
new_incomes    = pd.merge(df,grouped,how="inner",on="nomprov").loc[:, ["nomprov","renta_y"]]
##### Inner basically means that it will create a new column only, i.e intersection,
instead of 


df.loc[df.renta.isnull(),"renta"] = new_incomes.loc[df.renta.isnull(),"renta"].reset_index()
df.loc[df.renta.isnull(),"renta"] = df.loc[df.renta.notnull(),"renta"].median()

^ You see with the above, they also have a savegaurd in the form of at the end just taking the median to fill the remaining missing values after all the others have been checked. 

See how the indexing differs:
join : {â_˜innerâ_™, â_˜outerâ_™}, default â_˜outerâ_™. How to handle indexes on other axis(es). Outer for union and inner for intersection.

Remember that you can also fill a dataframe with the most common type:
df.loc[df.tiprel_1mes.isnull(),"tiprel_1mes"] = "A"

I personally haven't tried modus, but if there is such an option it migth be quite dangerous given that the modus might be null, and that is not the best way out.

Eternal Sunshine of the spotless mind.
Like Crazy. 
Amelie - 

This is pretty interesting, you feed it zeros and ones: 

max_ax = max((0, 1), key=lambda i: img.size[i])
after which it calculates the maximum img siz.e 

    # Emumerate adds a counter to the itterable, thus in this scenario we...
    # ... have the real id and a number of the id.
    
    for i, idee in enumerate(ids):


sys.path[0] gives you the directory where the current script resides:

import sys, os
script_dir = sys.path[0]
img_path = os.path.join(script_dir, '../y/img1.png')


http://mybinder.org:/repo/snowde/templates

This is an interesting where command, that says if equal, then x else y.

mom1 = np.where(mom1 > 0, 1, 0)

Interesting: You can create the below method to create a dataframe out of a group by object:

grouped = blue.groupby(["fyearq", "fqtr"],as_index=False).count()

This approach is much better in creating a dataframe ou tof a group by object:

grouped = blue.groupby(["fyearq", "fqtr"])

for key, item in grouped:
    print grouped.get_group(key), "\n\n"
    
Another method:

df.groupby('A').apply(lambda x: x)

NB the last value in brackets is inportant because you have to identify which values you want ot keep in the dataframe

grouped = blue.groupby(["fyearq", "fqtr"])[["mom"]]

bloom = train_new.reset_index()
bloom.reindex(train_new["id"])
train_new = train_new.fillna(a_mean)
id_df = train_new[col].groupby('id').agg([np.mean, np.std, len]).reset_index()

grouped = merged.groupby(["fyearq", "fqtr", "ticker"]).agg([np.mean, np.sum, np.std, len]).reset_index()


for o in range(len(train_new)):
id_df = bloom[col].groupby('id')
a_mean = id_df.mean().reset_index()
train_new = train_new.fillna(a_mean)
sum(train_new.isnull().sum(axis=1))

train["std"] = train.groupby('id').y.std()
corr = train.corr().ix["y", "std"]

merged = pd.merge(price, fiscal, on="date", how="outer")


# NB ix is the pandas slicer. 
fire1 = fire.ix[[:0]]
fire2 = fire.ix[[2:]]
newfire = pd.concat([fire1, fire2], axis=1)

# NB this drops the second row heade
fire.columns = fire.columns.droplevel(1)

-> This is interesting, reminds you that you have to 
pd.merge(left, right, left_on='key1', right_on='key2')

# Interesting, merge and joing essentially means the same thing,
# slightly differnt way of writing it out. 

And then concatante the one that just copies and paste. 


Here I am first creating a separate datafrane, 

d = pd.DataFrame()
dates = pd.date_range('20040101', '20170101')
s = pd.Series(dates)
d["fdate"] = s

This is how you can create awesome files from /// to yyyymmdd
TEXT(A1,"yyyymmdd")  and then drag it down. 

There is a way to play around with excel in pandas.

import openpyxl

d = pd.DataFrame()
dates = pd.date_range('2004/01/01', '2017/01/01')

mask in dataframe is like where. (IF THEN statement)

The statement I am looking for is ht emap function, which maps for each x value.

prices["cusip"]= prices["cusip"].map(lambda x: x[:5] + '10')

MAP FOR SERIES, APPLY FRO FRAME

apply is on id arrays

Wow this is an awesom .isin list code:

prices = prices[~prices["tic"].isin(tickel)]

Wow there actually is a way to get numpy into dataframe:

df = pd.DataFrame({'mom1':mom1.tolist()})

df = pd.DataFrame(numpyobject)

Here is an intresting location function, this helps to achieve good things:

d["close"] = new_group["close"].loc[new_group["year"]>"2004"]

When you get this issue::: 

only integers, slices (`:`), ellipsis (`â_|`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

Then you can fix it by first specifying a dataframe beforeyou do anything else:

mom2 = pd.DataFrame()
mom2['price_above'] = np.where(values > 0, 1, 0)

Melt is one of the most beautiful function that I have ever seen, it stacks columns.

keys = [c for c in mom_vol_n if c.startswith('mom_vol_')]
new = pd.melt(mom_vol_n, value_vars=keys, value_name='mom_v')


NB when you are running loops over a dictionary in python 2 then use
for i, v in varlist.iteritems():

in python 3 you only have to use:

for i, v in varlist.items():

But, you have to inlcue items. 


You can find dtyoe of column doing the following:

pandas["column"].dtype
no ()


Awesome this code is good for appen in for :

   framed = np.append(framed, [train_predictions], axis=0)


To force division to be float

from __future__ import division

To pass on certain type of warnings:

def warn(*args, **kwargs): pass


For max values
numpy.set_printoptions(threshold='nan')


load ./zero_cleaner.py
This is an interesting line of code, it actually helps you to visualy load any .py into your document.

There is even a way to pass variables between netbooks, see this link:

https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/

Awesome you can us the follwoing magic command to write a cell to a python 
document: 

%%writefile pythoncode.py


This is a way to load in your py code without visually showing the code:

%run zero_cleaner.py

Also if you compilef the class file in the same python version you can use it as a class by chanign it to .pyc and just imorting it with just using the name
zero_cleaner

normalized_df=(coy-coy.min())/(coy.max()-coy.min())

The below is a long approach that simply does what is done above:

scaler = preprocessing.MinMaxScaler()
scaled = scaler.fit_transform(values)
coy = pd.DataFrame(scaled)

Interesting, you can .value an entire dataframe:

values = coy.values

NB when merging items, makes sure that you are mergin items of the same type.

You normally import in string, meaning that your next iports should also be in string. You can import one file in string and the other one in auto format i.e. leave to default, and then group the frame together later on.

This is how to import string 

fiscal = pd.read_csv("fiscal_bo.csv", dtype=str)

how : {â_˜anyâ_™, â_˜allâ_™}
any : if any NA values are present, drop that label
all : if all values are NA, drop that label

How to install talib package: 
ta-lib: 0.4.9-np19py27_0 quantopian

conda install -c quantopian ta-lib=0.4.9

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

This is probably a better way to write the merged file or any big file, just cut it off :

data = pd.merge(data, nRuta_SAK, 
                            how='left',
                            left_on=['Ruta_SAK'], 
                            right_on=['Ruta_SAK'],
                            left_index=False, right_index=False, sort=True,
                            suffixes=('_x', '_y'), copy=False) 

This is what I have: 

py27 -> py27na (This is the one without talib possibly?) 

ta-lib: 0.4.9-np19py27_0 quantopian
I found a way to clone an environement: this means that you can have similar running packages:

dereksnow$ conda create -n py27na --clone py27

How to insert at a specific locatoin:

coy_df.insert(0,"rdq",rdq)

Range is still the best approach to make a long list over a certain number 
range(X.shape[1])

In terms of website: 

Nameserver changes are heavier, dns is less heavy.

Nameserver means that the full domain sits with someone else.

Nameserver changes host.

DNS only gives another the right to render I believe.

I actually want to read up on some of these stuff, and find out how much they are different. 

The following is some interesting Conda Code:

conda search python

conda install python=xxx.xxx

python --version

conda create -n py35 python=3.5 anaconda

conda update python

source activate py36

source deactivate py36

conda create -n py27 python=2.7 anaconda
conda create -n 27clean python=2.7 anaconda
source activate py27
conda install notebook ipykernel
ipython kernel install --user
ipython kernel install --user --name t35
(I screwed this one up)

How to open bash files for editing:

touch ~/.bash_profile; open ~/.bash_profile

source activate py27clean

You have to add the anaconda at the back otherwise the thing gets confused. 

# To activate this environment, use:
# > source activate 27clean
#
# To deactivate this environment, use:
# > source deactivate 27clean
#

# To activate this environment, use:
# > source activate 27
#
# To deactivate this environment, use:
# > source deactivate 27talib


Another thing to be very cautious of is what kernel you are using in ipython.

So far I only have these environments: 
27clean, 27talib etc that is worth using. 

inputs = {
    'open': open_1,
    'high': high,
    'low': low,
    'close': close,
    'volume': volume
}
# Wow this is interesting, you can specify the input arrays in the hope that
# the function will automatically pick them up.
# I like this idea, everything will process much quicker.



What this is awesome, there is actually a way to loop in parrallel, i.e. a simultaneous loop.

for f, b in zip(foo, bar):
    print(f, b)
    
You don't ever really have to change the variable name, you can just change the column name:

saved[lab] = i(inputs)

The approach to get_dummmies. 

Fuck that, i just created my own dummies:

    saved[b+'false_'+ cusi] = np.where(i(inputs) >99,1,0)
    saved[b+'true'+ cusi] = np.where(i(inputs) <-99,1,0)


But there is another way to do get dummies and this is with this procedure:

cols_to_transform = ['year', 'qtr']
frame = pd.get_dummies(avg, columns = cols_to_transform)
frame = pd.concat((frame, avg), axis=1)

This is how to fill numpy nan's with zeros:

here[np.isnan(here)] = 0

At the moment I have done min max scaling because it is noteably good for neural networks. If I find that the results are not to good, I might also look at producing standarisation techniques. 

You can't really do them together I have thought about that now. 

Can use command plus the arrow buttons to get to the end of along line. A lot like excel in fact. 

That is very interesting, instead of forcing a shit load of new variables, you can instead store all these "variables" away in a dictionary, this leads to much more efficent code. It kind of looks like a dataframe when you do this. 

NB there is a big distinction between append in lists: 

Append is essentially in a loop where each new item such as "burger", "pizza" etc gets appended one after the other in a long list.

However, when you have a pre-established list: list area = ["burger", "pizza"]
and blue = ["diamond", "liament"] then:
blue = blue + area to keep making a long list where one follows the other.

So the above should not use the append function. 

However, it does seem that you can add the function extend instead of the plus:

x.extend([4, 5])

This is how to do that fancy formatting printing:

print("%d. feature %d and %str (%f)" % (f + 1, indices[f], list(X)[indices[f]], importances[indices[f]]))


It's simply pointless to create variable variable names. Why?

They are unnecessary: You can store everything in lists, dictionarys and so on
They are hard to create: You have to use exec or globals()

How to select a certain cell in a pandas dataframe:

train.iloc[0]["photos"]

JSON's upload is tricky in that it is a little bit differnt to read:

# train = pd.read_json("train.json", "r")
You need the r, otherwise it does not give you all the level that you need. 

NB you can change the bash profile in just one line:
'

Steps:
Connect VPN
ssh dsno800@ml.cer.auckland.ac.nz
export PATH="/home/dsno800/anaconda3/bin:$PATH"
anaconda (TEST)

dsno800@ml.cer.auckland.ac.nz$$$$$$ jupyter notebook --no-browser --port=8890

For some reason, it only seems like you have to run the below code once
-----------------------------------------------------------------------------
derek:~ dereksnow$$$$$ ssh -N -f -L localhost:8888:localhost:8890 dsno800@ml.cer.auckland.ac.nz

You have to indeed follow all the steps
1 ,bluh. 

A cool way to do your password in one line:

echo 'D13sn33uman!' | sudo -S apt-get install unzip

There is an interesting service you can use to upload to a server.

Follow these steps:

How to move files around

ls into folder then
mv file new_folder 


! cd ~/Notebooks
# ! wget -x -c --load-cookies cookies.txt -P data -nH --cut-dirs=5 https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip
! cd ~/Notebooks/data
! ls

! wget -x -c --load-cookies cookies.txt -P data -nH --cut-dirs=5 https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip

This is easy, it lists the header of the dataframe:

listed = list(df)

And this is how you can change the header with a list:
df.columns = listed

Use the one or the other depending on which one is fucking up:

from sklearn.cross_validation import train_test_split
from sklearn.model_selection import cross_validation

http://localhost:8888/tree/Lending/www.kaggle.com/wendykan/lending-club-loan-data/downloads




To install Python package from github, you need to clone that repository.

git clone https://github.com/jkbr/httpie.git
Then just run the setup.py file from that directory,

sudo python setup.py install

This is pretty interesting, you can get more head by incorporating the number in the head:
coeffdf.head(10)








































































































```
